#include <vector>
#include <random>
#include <fstream>
#include <sstream>
#include <string>
#include <iostream>
#include <unordered_map>
#include "json.hpp"  // nlohmann::json single-header

using json = nlohmann::json;

template<typename... Args> void print(const Args&... args) { (std::cout << ... << args) << std::endl; }

class BPETokenizer {
public:
    std::unordered_map<std::string, int> vocab;
    std::unordered_map<std::string, int> merges;

    void load_vocab(const std::string& path) {
        std::ifstream in(path);
        if (!in.is_open()) {
            std::cerr << "Error: cannot open vocab file '" << path << "'\n";
            std::exit(EXIT_FAILURE);
        }
        json j; in >> j;
        for (auto it = j.begin(); it != j.end(); ++it)
            vocab[it.key()] = it.value().get<int>();
    }

    void load_merges(const std::string& path) {
        std::ifstream in(path);
        if (!in.is_open()) {
            std::cerr << "Error: cannot open merges file '" << path << "'\n";
            std::exit(EXIT_FAILURE);
        }
        std::string line;
        std::getline(in, line);  // skip header
        int rank = 0;
        while (std::getline(in, line)) {
            std::istringstream iss(line);
            std::string a, b;
            iss >> a >> b;
            if (!a.empty() && !b.empty())
                merges[a + " " + b] = rank++;
        }
    }

    std::vector<std::string> encode_phrase(const std::string& raw) {
        std::string text = raw;
        if (text.empty() || text[0] != ' ') text = " " + text;
        std::vector<std::string> tokens;
        for (size_t i = 0; i < text.size(); ++i)
            tokens.push_back(text.substr(i, 1));
        while (true) {
            int best_rank = std::numeric_limits<int>::max();
            int best_i = -1;
            for (size_t i = 0; i + 1 < tokens.size(); ++i) {
                std::string key = tokens[i] + " " + tokens[i + 1];
                auto it = merges.find(key);
                if (it != merges.end() && it->second < best_rank) {
                    best_rank = it->second;
                    best_i = static_cast<int>(i);
                }
            }
            if (best_i < 0) break;
            tokens[best_i] += tokens[best_i + 1];
            tokens.erase(tokens.begin() + best_i + 1);
        }
        for (auto& t : tokens)
            if (!vocab.count(t)) t = "<unk>";
        return tokens;
    }

    std::vector<int> encode_ids(const std::string& phrase) {
        auto toks = encode_phrase(phrase);
        std::vector<int> ids;
        ids.reserve(toks.size());
        for (auto& t : toks)
            ids.push_back(vocab.count(t) ? vocab[t] : vocab["<unk>"]);
        return ids;
    }
};

// ---------------- Embedding Class ----------------
class Embedding {
public:
    Embedding(int vocab_size, int embed_dim)
        : vocab_size_(vocab_size), embed_dim_(embed_dim) {
        // Random initialization
        std::mt19937 gen(std::random_device{}());
        std::normal_distribution<float> dist(0.0f, 0.1f);
        weights_.resize(vocab_size_);
        for (int i = 0; i < vocab_size_; ++i) {
            weights_[i].resize(embed_dim_);
            for (int j = 0; j < embed_dim_; ++j)
                weights_[i][j] = dist(gen);
        }
    }

    // Optionally load pretrained embeddings from a text file: one line per token: <id> <v1> <v2> ...
    void load_from_file(const std::string& path) {
        std::ifstream in(path);
        if (!in.is_open()) {
            std::cerr << "Error: cannot open embedding file '" << path << "'\n";
            std::exit(EXIT_FAILURE);
        }
        int id;
        std::string line;
        while (std::getline(in, line)) {
            std::istringstream iss(line);
            iss >> id;
            if (id < 0 || id >= vocab_size_) continue;
            for (int j = 0; j < embed_dim_; ++j)
                iss >> weights_[id][j];
        }
    }

    // Lookup single token ID
    const std::vector<float>& operator[](int token_id) const {
        return weights_[token_id];
    }

    // Lookup a sequence of token IDs
    std::vector<std::vector<float>> lookup(const std::vector<int>& ids) const {
        std::vector<std::vector<float>> out;
        out.reserve(ids.size());
        for (int id : ids)
            out.push_back(weights_[id]);
        return out;
    }

    int vocab_size() const { return vocab_size_; }
    int embed_dim() const { return embed_dim_; }

private:
    int vocab_size_;
    int embed_dim_;
    std::vector<std::vector<float>> weights_;
};

int main(int argc, char* argv[]) {
    BPETokenizer tok;
    tok.load_vocab("database/vocab.json");
    tok.load_merges("database/merges.txt");

    // Prepare input phrase
    std::string phrase = argc > 1 ? std::string(argv[1])
        : "Hello world from BPE RNN!";
    auto ids = tok.encode_ids(phrase);

    // Create embedding
    const int EMBED_DIM = 128;
    Embedding embed(tok.vocab.size(), EMBED_DIM);

    // Optionally load pretrained embeddings:
    // embed.load_from_file("path/to/pretrained.vec");

    // Perform lookup
    auto vectors = embed.lookup(ids);

    // Print out embeddings for each token
    print("Token IDs and their embeddings (first 5 dims):");
    for (size_t i = 0; i < ids.size(); ++i) {
        std::cout << "ID=" << ids[i] << ": [";
        for (int j = 0; j < std::min(5, EMBED_DIM); ++j) {
            std::cout << vectors[i][j] << (j + 1 < 5 ? ", " : "");
        }
        std::cout << "]\n";
    }

    return 0;
}
